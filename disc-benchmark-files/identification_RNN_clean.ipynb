{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "749d671f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Load required packages\n",
    "import torch\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['text.usetex'] = False\n",
    "import numpy as np\n",
    "\n",
    "# register acceleration devices\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load Benchmark Data\n",
    "\n",
    "out = np.load('training-val-test-data.npz')\n",
    "th_data = out['th'] #th[0],th[1],th[2],th[3],...\n",
    "u_data = out['u'] #u[0],u[1],u[2],u[3],...\n",
    "\n",
    "from sklearn import model_selection\n",
    "u_train, u_val, y_train, y_val= model_selection.train_test_split(u_data, th_data, shuffle=False, test_size=0.3,random_state=42)\n",
    "# Standardization\n",
    "u_mean, u_std = np.mean(u_train),np.std(u_train)\n",
    "y_mean, y_std = np.mean(y_train),np.std(y_train)\n",
    "\n",
    "\n",
    "### Data to train and validate on ###:\n",
    "u_train = (u_train-u_mean)/u_std #normalize\n",
    "y_train = (y_train-y_mean)/y_std\n",
    "\n",
    "u_val = (u_val-u_mean)/u_std\n",
    "y_val = (y_val-y_mean)/y_std\n",
    "\n",
    "convert = lambda x: [torch.tensor(xi,dtype=torch.float64,device=device) for xi in x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c2dac8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m     Loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((Yin[:,n_burn:] \u001b[38;5;241m-\u001b[39m Yout[:,n_burn:])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     92\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 93\u001b[0m     Loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     94\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[1;32mc:\\Users\\Kai\\anaconda3\\envs\\ml4sc\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    628\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Kai\\anaconda3\\envs\\ml4sc\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Kai\\anaconda3\\envs\\ml4sc\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def make_OE_init_state_data(udata, ydata, nf=100, n_encode=20):\n",
    "    U = []\n",
    "    Y = []\n",
    "    hist = []\n",
    "    for k in range(nf+n_encode,len(udata)+1):\n",
    "        hist.append(np.concatenate((udata[k-nf-n_encode:k-nf], ydata[k-nf-n_encode:k-nf]))) #f)\n",
    "        U.append(udata[k-nf:k]) #f)\n",
    "        Y.append(ydata[k-nf:k]) #f)\n",
    "    return np.array(hist), np.array(U), np.array(Y)\n",
    "\n",
    "nfuture = 35\n",
    "n_encode = 15\n",
    "convert = lambda x: [torch.tensor(xi, dtype=torch.float64,device=device) for xi in x]\n",
    "histtrain, Utrain, Ytrain = convert(make_OE_init_state_data(u_train, y_train, nf=nfuture,            n_encode=n_encode))\n",
    "histval,   Uval,   Yval   = convert(make_OE_init_state_data(u_val,   y_val,   nf=len(u_val)-n_encode, n_encode=n_encode))\n",
    "\n",
    "class simple_encoder_RNN(nn.Module):\n",
    "    def __init__(self, n_hidden_encoder, n_hidden_states, n_hidden_h2o, n_encoder=20):\n",
    "        super(simple_encoder_RNN, self).__init__()\n",
    "        self.n_hidden_states = n_hidden_states\n",
    "        self.input_size = 1\n",
    "        self.output_size = 1\n",
    "        net = lambda n_in,n_out, n_hidden: nn.Sequential(nn.Linear(n_in,n_hidden),nn.ReLU(),nn.Linear(n_hidden,n_out)) #short hand for a 1 hidden layer NN\n",
    "        self.rnn = nn.RNN(input_size=self.input_size,  hidden_size=n_hidden_states,  batch_first=True).double() #i)\n",
    "        self.h2o = net(n_hidden_states + self.input_size, self.output_size, n_hidden_h2o).double() #i)\n",
    "        self.hEncoder = net(n_encoder*2,n_hidden_states,n_hidden_encoder).double()\n",
    "\n",
    "    def forward(self, inputs, hist):\n",
    "        h0 = self.hEncoder(hist)\n",
    "        hiddens, h_n = self.rnn(inputs[:,:,None],h0[None,:,:]) #i)\n",
    "        combined = torch.cat((hiddens,inputs[:,:,None]),dim=2) #i)\n",
    "        if False: #two ways to solve this #i)\n",
    "            y_predict = [] #i)\n",
    "            for i in range(combined.shape[1]):  #i)\n",
    "                y_predict.append(self.h2o(combined[:,i,:])[:,0])  #i)\n",
    "            y_predict = torch.stack(y_predict,dim=1)  #i)\n",
    "        else: #this is faster but more complex #i)\n",
    "            #reshape from (N_batch, N_time, N_hidden) to (N_batch*N_time, N_hidden) #i)\n",
    "            h2o_input = combined.view(-1,self.n_hidden_states+self.input_size) #i)\n",
    "            #apply and reshape from (N_batch*N_time,1) to (N_batch,N_time) #i)\n",
    "            y_predict =  self.h2o(h2o_input).view(inputs.shape[0],inputs.shape[1])  #new #i)\n",
    "\n",
    "        return y_predict\n",
    "\n",
    "class simple_lstm(nn.Module):\n",
    "    def __init__(self, n_hidden_states, n_encoder=20):\n",
    "        super(simple_lstm, self).__init__()\n",
    "        self.n_hidden_states = n_hidden_states\n",
    "        self.input_size = 1\n",
    "        self.output_size = 1\n",
    "        net = lambda n_in,n_out: nn.Sequential(nn.Linear(n_in,40),nn.Sigmoid(),nn.Linear(40,n_out)) #short hand for a 1 hidden layer NN\n",
    "        self.lstm = nn.LSTM(input_size=self.input_size,  hidden_size=n_hidden_states,  batch_first=True).double() #i)\n",
    "        self.h2o = net(n_hidden_states + self.input_size, self.output_size).double() #i)\n",
    "        self.hEncoder = net(n_encoder*2,n_hidden_states).double()\n",
    "        self.cEncoder = net(n_encoder*2,n_hidden_states).double()\n",
    "\n",
    "    def forward(self, inputs, hist):\n",
    "        h0 = self.hEncoder(hist)\n",
    "        c0 = self.cEncoder(hist)\n",
    "        hiddens, (h_n, c_n) = self.lstm(inputs[:,:,None],(h0[None,:,:],c0[None,:,:])) #i)\n",
    "        #hiddens, h_n = self.lstm(inputs[:,:,None]) #i)\n",
    "        combined = torch.cat((hiddens,inputs[:,:,None]),dim=2) #i)\n",
    "        if False: #two ways to solve this #i)\n",
    "            y_predict = [] #i)\n",
    "            for i in range(combined.shape[1]):  #i)\n",
    "                y_predict.append(self.h2o(combined[:,i,:])[:,0])  #i)\n",
    "            y_predict = torch.stack(y_predict,dim=1)  #i)\n",
    "        else: #this is faster but more complex #i)\n",
    "            #reshape from (N_batch, N_time, N_hidden) to (N_batch*N_time, N_hidden) #i)\n",
    "            h2o_input = combined.view(-1,self.n_hidden_states+self.input_size) #i)\n",
    "            #apply and reshape from (N_batch*N_time,1) to (N_batch,N_time) #i)\n",
    "            y_predict =  self.h2o(h2o_input).view(inputs.shape[0],inputs.shape[1])  #new #i)\n",
    "\n",
    "        return y_predict\n",
    "\n",
    "n_burn = 0\n",
    "#model = simple_lstm(n_hidden_states=10,n_encoder=n_encode)\n",
    "model = simple_encoder_RNN(n_hidden_encoder=32,n_hidden_states=3,n_hidden_h2o=32,n_encoder=n_encode)\n",
    "model = model.to(device=device) # Move the model to the GPU\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
    "batch_size = 64\n",
    "ids = np.arange(len(Utrain),dtype=int)\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    for i in range(0,len(Utrain),batch_size):\n",
    "        ids_now = ids[i:i+batch_size]\n",
    "        Uin = Utrain[ids_now]\n",
    "        histin = histtrain[ids_now]\n",
    "        Yout = model.forward(inputs=Uin,hist=histin)\n",
    "        Yin = Ytrain[ids_now]\n",
    "        Loss = torch.mean((Yin[:,n_burn:] - Yout[:,n_burn:])**2)\n",
    "        optimizer.zero_grad()\n",
    "        Loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        Loss_val = torch.mean((model(inputs=Uval,hist=histval)[:,n_burn:] - Yval[:,n_burn:])**2)**0.5\n",
    "        Loss_train = torch.mean((model(inputs=Utrain,hist=histtrain)[:,n_burn:] - Ytrain[:,n_burn:])**2)**0.5\n",
    "        print(f'epoch={epoch}, Validation Loss={Loss_val.item():.2%}, Train Loss={Loss_train.item():.2%}, Validation Loss absolute={Loss_val.item() * y_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542b727",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.plot((Yval[0]*y_std + y_mean).cpu())\n",
    "    model.eval()\n",
    "    plt.plot((model(inputs=Uval,hist=histval)[0] * y_std + y_mean).cpu(),'--')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('y')\n",
    "    plt.xlim(0,250)\n",
    "    plt.legend(['real','predicted'])\n",
    "    plt.show()\n",
    "    model.eval()\n",
    "    plt.plot(y_std*np.mean(((Ytrain-model(inputs=Utrain,hist=histtrain))).cpu().numpy()**2,axis=0)**0.5) #average over the error in batch\n",
    "    plt.title('batch averaged time-dependent error')\n",
    "    plt.ylabel('error')\n",
    "    plt.xlabel('i')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    model.eval()\n",
    "    histtrain, Utrain, Ytrain = convert(make_OE_init_state_data(u_train, y_train, nf=nfuture,            n_encode=n_encode))\n",
    "    histval,   Uval,   Yval   = convert(make_OE_init_state_data(u_val,   y_val,   nf=len(u_val)-n_encode, n_encode=n_encode))\n",
    "    histTrainEval, UTrainEval, YTrainEval = convert(make_OE_init_state_data(u_train, y_train, nf=len(u_train)-n_encode, n_encode=n_encode))\n",
    "\n",
    "    Ytrain = YTrainEval[0] * y_std + y_mean\n",
    "    Ytrain_pred = model(inputs=UTrainEval,hist=histTrainEval)[0] * y_std + y_mean\n",
    "    print('train prediction errors:')\n",
    "    print('RMS:', torch.mean((Ytrain_pred-Ytrain)**2)**0.5,'radians')\n",
    "    print('RMS:', torch.mean((Ytrain_pred-Ytrain)**2)**0.5/(2*np.pi)*360,'degrees')\n",
    "    print('NRMS:', torch.mean((Ytrain_pred-Ytrain)**2)**0.5/y_std*100,'%')\n",
    "    res = Ytrain_pred - Ytrain #b)\n",
    "    plt.figure(figsize=(12,5)) #b)\n",
    "    plt.plot(Ytrain.cpu()) #b)\n",
    "    plt.plot(res.cpu()) #b)\n",
    "    plt.xlabel('time index'); plt.ylabel('y'); plt.legend(['y','y - y predic'])#b)\n",
    "    #plt.xlim(0,250)\n",
    "    plt.show() #b)\n",
    "\n",
    "    Yval = Yval[0] * y_std + y_mean\n",
    "    Yval_pred = model(inputs=Uval,hist=histval)[0] * y_std + y_mean\n",
    "    res = Yval_pred - Yval #b)\n",
    "    plt.figure(figsize=(12,5)) #b)\n",
    "    plt.plot(Yval.cpu()) #b)\n",
    "    plt.plot(res.cpu()) #b)\n",
    "    plt.xlabel('time index'); plt.ylabel('y'); plt.legend(['y','y - y predic'])#b)\n",
    "    plt.show() #b)\n",
    "    print('validation prediction errors:')\n",
    "    print('RMS:', torch.mean((Yval_pred-Yval)**2)**0.5,'radians')\n",
    "    print('RMS:', torch.mean((Yval_pred-Yval)**2)**0.5/(2*np.pi)*360,'degrees')\n",
    "    print('NRMS:', torch.mean((Yval_pred-Yval)**2)**0.5/Yval.std()*100,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up simulation for identified net\n",
    "#### What's done with the na and nb here???\n",
    "# Simulation Test\n",
    "out = np.load('/content/training-val-test-data.npz')\n",
    "th_train = out['th'] #th[0],th[1],th[2],th[3],...\n",
    "u_train = out['u'] #u[0],u[1],u[2],u[3],...\n",
    "\n",
    "data = np.load('/content/hidden-test-simulation-submission-file.npz')\n",
    "u_test = data['u']\n",
    "th_test = data['th'] #only the first 50 values are filled the rest are zeros\n",
    "\n",
    "\n",
    "def simulation_IO_model(f, ulist, ylist, skip=50):\n",
    "    upast = ulist[skip-n_encode:skip].tolist() #good initialization\n",
    "    ypast = ylist[skip-n_encode:skip].tolist()\n",
    "    hist = np.concatenate([(upast-u_mean)/u_std,(ypast-y_mean)/y_std],axis=0)\n",
    "    inputs = (np.array(ulist[skip:].tolist()) - u_mean)/u_std\n",
    "    hist, inputs = convert([hist[None,:], inputs[None,:]])\n",
    "    ypred = f(inputs,hist).cpu().detach().numpy() * y_std + y_mean\n",
    "    Y = np.concat([ylist[:skip],ypred])\n",
    "    return np.array(Y)\n",
    "\n",
    "skip = n_encode\n",
    "model.eval()\n",
    "th_train_sim = simulation_IO_model(lambda x, hist: model.forward(inputs=x,hist=hist)[0], u_train, th_train, skip=skip)\n",
    "print('trained simulation errors:')\n",
    "print('RMS:', np.mean((th_train_sim[skip:]-th_train[skip:])**2)**0.5,'radians')\n",
    "print('RMS:', np.mean((th_train_sim[skip:]-th_train[skip:])**2)**0.5/(2*np.pi)*360,'degrees')\n",
    "print('NRMS:', np.mean((th_train_sim[skip:]-th_train[skip:])**2)**0.5/th_train.std()*100,'%')\n",
    "# Plotting\n",
    "res = th_train_sim[skip:] - th_train[skip:] #b)\n",
    "plt.figure(figsize=(12,5)) #b)\n",
    "plt.plot(th_train) #b)\n",
    "plt.plot(res) #b)\n",
    "plt.xlabel('time index'); plt.ylabel('y'); plt.legend(['y','y - y sim'])#b)\n",
    "plt.show() #b)\n",
    "\n",
    "skip = 50\n",
    "th_test_sim = simulation_IO_model(lambda x,hist: model(x,hist)[0], u_test, th_test, skip=skip)\n",
    "print('test simulation errors:')\n",
    "print('RMS:', np.mean((th_train_sim[skip:]-th_train[skip:])**2)**0.5,'radians')\n",
    "print('RMS:', np.mean((th_train_sim[skip:]-th_train[skip:])**2)**0.5/(2*np.pi)*360,'degrees')\n",
    "print('NRMS:', np.mean((th_train_sim[skip:]-th_train[skip:])**2)**0.5/th_train.std()*100,'%')\n",
    "# Plotting\n",
    "res = th_test_sim[skip:] - th_test[skip:] #b)\n",
    "plt.figure(figsize=(12,5)) #b)\n",
    "plt.plot(th_test) #b)\n",
    "plt.plot(res) #b)\n",
    "plt.xlabel('time index'); plt.ylabel('y'); plt.legend(['y','y - y sim'])#b)\n",
    "plt.show() #b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4sc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
